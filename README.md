CompanionOS (Avalon)

CompanionOS is a local, privacy-first AI companion system designed to run entirely on your machine.
Unlike cloud-based assistants, CompanionOS gives you full ownership of your data, memory, and tools while still behaving like a modern, intelligent assistant.

Avalon = Artificial Virtual Assistant with Logical and Operational Navigation

â¸»

ğŸš€ What Is CompanionOS?

CompanionOS is an AI operating loop, not just a chatbot.

It combines:
	â€¢	Local LLM inference
	â€¢	Structured long-term memory
	â€¢	Persona-driven behavior
	â€¢	Safety & quality enforcement
	â€¢	Actionable tools (alerts/reminders)

All running offline using Docker.

â¸»

ğŸ› ï¸ Tech Stack

Backend â€” The Brain
	â€¢	Language: Python 3.11+
	â€¢	Framework: FastAPI
	â€¢	AI Engine: Ollama (local LLMs like Llama 3)
	â€¢	Database: SQLite (local, file-based)
	â€¢	Key Libraries:
	â€¢	pydantic â€“ strict schema validation
	â€¢	requests â€“ Ollama API communication
	â€¢	pytz â€“ timezone-aware scheduling

Frontend â€” The Face
	â€¢	Framework: Next.js 14 (App Router)
	â€¢	Language: TypeScript
	â€¢	Styling: Tailwind CSS
	â€¢	State Management: React hooks (useState, useEffect)

Infrastructure
	â€¢	Docker Compose
	â€¢	Backend (FastAPI)
	â€¢	Frontend (Next.js)
	â€¢	Ollama (LLM runtime)

Run everything with one command.

â¸»

ğŸ—ï¸ Architecture Overview

The Core Loop
	1.	User sends a message from the frontend
	2.	FastAPI backend:
	â€¢	Loads persona configuration
	â€¢	Retrieves relevant memory (session + global)
	â€¢	Builds a deterministic system prompt
	3.	Prompt is sent to Ollama
	4.	LLM generates a draft response
	5.	Draft is passed to the Judge Agent
	6.	Final response is returned to the user

â¸»

ğŸ›¡ï¸ The Judge System (Safety Layer)

Every assistant response is reviewed by a Judge Agent:
	â€¢	Ensures persona consistency
	â€¢	Prevents unsafe or manipulative outputs
	â€¢	Decides:
	â€¢	PASS â€“ response is acceptable
	â€¢	REWRITE â€“ minimal safe correction
	â€¢	BLOCK â€“ refuse with explanation

This guarantees quality + alignment, even with local models.

â¸»

ğŸ” The Pipeline (Post-Chat Intelligence)

After each chat turn, a background pipeline runs:

ğŸ§  Memory Agent (MX1)
	â€¢	Extracts stable user facts and preferences
	â€¢	Stores them as structured memory
	â€¢	Uses confidence thresholds to avoid noise
	â€¢	Updates session summaries every N messages

ğŸ› ï¸ Tools Agent
	â€¢	Scans conversation for actionable intent
	â€¢	Example:
â€œRemind me tomorrow to apply to 3 companiesâ€
	â€¢	Converts intent into structured tool actions

â¸»

â° Alert System (Tools MVP)

The first implemented tool is in-app alerts:
	1.	Extraction
	â€¢	LLM detects time-based or task-based intent
	2.	Storage
	â€¢	Python calculates exact due_at
	â€¢	Alert stored in SQLite
	3.	Notification
	â€¢	Frontend polls backend periodically
	â€¢	When alert is due â†’ toast notification
	4.	User Control
	â€¢	Acknowledge or dismiss alerts from UI

â¸»

âœ¨ Key Features
	â€¢	ğŸ  Local & Private
No data leaves your machine. No cloud APIs.
	â€¢	ğŸ§  Long-Term Memory
Remembers goals, preferences, and plans across sessions.
	â€¢	ğŸ­ Personas
Switch personalities (Coach, Mentor, Calm) via config files.
	â€¢	â° Active Tools
Alerts & reminders extracted directly from conversation.
	â€¢	ğŸ›¡ï¸ Self-Correction
Judge agent enforces safety and persona consistency.
	â€¢	ğŸ“¦ One-Command Setup
Fully dockerized for easy onboarding.




â–¶ï¸ Running CompanionOS

Prerequisites
	â€¢	Docker
	â€¢	Docker Compose
	â€¢	Ollama installed locally

Start Everything

ğŸ¯ Project Goals
	â€¢	Build a sovereign AI companion
	â€¢	Avoid cloud dependency and vendor lock-in
	â€¢	Provide a framework for:
	â€¢	Personas
	â€¢	Memory
	â€¢	Tools
	â€¢	Local agents

This is designed as a foundation, not a one-off demo.

â¸»

ğŸ”® Future Work
	â€¢	More tools (calendar, tasks, notes)
	â€¢	Plugin-style tool marketplace
	â€¢	Better scheduling & background workers
	â€¢	Multi-agent routing
	â€¢	Mobile UI
